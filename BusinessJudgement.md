# Data Driven Approach to Homeaglow’s improvement.

**1\. What target metric and secondary metrics should we track?**  
Target Metric :  
   \- Revenue per Customer (New and Old Customers): This will show the financial impact of increasing the membership price , we could later compare the revenue per customer for the new and old customers to see the impact of the price increase or even use hypothesis testing to see if the price increase has a significant impact on the revenue per customer.  
Secondary Metrics :  
   \- Conversion Rate of Customers (New and Old Customers): Percentage of customers interested in signing up for the $60/month membership and the $50/month membership.  
   \- Retention Rate: The percentage of customers who keep their subscription after a period of time (1, 3, or 6 months).  
   \- Customer Lifetime Value : also known as CLTV, will be used to track the total revenue generated by a customer over their subscription lifetime.  
   \- Churn Rate: will be used to track the rate at which customers cancel their membership.  
   \- "Discounted Cleaning" Utilization Rate: Measures how frequently members use the discounted cleaning service (this will be used to track the impact of the price increase on the utilization of the discounted cleaning service).  
   \- Customer Satisfaction (via NPS or survey scores): gives us a measure of customer sentiment and experience after signup.  
   \- Acquisition Cost per Customer: Measures whether a higher price impacts marketing efficiency.  
**2\. How can we conduct an experiment to test this opportunity?**  
Experimental Design:  
   Objective: Is to Determine if increasing to the $60/month price point will bring forth a higher profitability without significantly reducing customer acquisition, retention, or satisfaction.  
   for AB testing we will need to create two groups:  
      Control Group: New customers offered the $50/month membership.  
      Treatment Group: New customers offered the $60/month membership.  
   Random Assignment: Randomly assign the new customers to the control or treatment groups based on even distribution to avoid bias.  
   Sample Size: Use a power analysis calculator to set the minimum number of customers needed to detect statistically significant differences in target metrics (e.g., revenue or retention rates). Consider a power of 80% and a significance level of 5%.  
   Engineering Implementation:  
   Frontend Changes: Dynamically display the $50 or $60 price during signup based on the groups.  
   Backend Updates: Implement logging to track groups, conversions, cleaning usage, and customer retention.  
   Data Pipeline: Ensure that all experiment-related data flows into an analytics platform for easy tracking and reporting.  
   Sample Sizing: Use the historical data to estimate baseline metrics (e.g., conversion rate, retention).  
**3\. How long should we run this experiment?**  
Minimum Duration: The experiment should run for a period long enough to capture at least one complete billing cycle (1 month) and a buffer for delayed signups to detect retention or churn trends. according to research a period of 4–6 weeks is a good starting point.  
Key Considerations:  
make sure to have an adequate sample size for statistical significance.  
Allow time for behavioral trends (e.g., churn or cleaning usage) to stabilize.  
**4\. What’re the downsides of running the experiment too long?**  
Revenue Loss: Delaying the rollout of a successful price increase could reduce potential profits.  
Market Exposure: Competitors or customers may notice the varying prices, leading to brand trust issues.  
External Factors: Prolonged experiments increase the chance that external factors (e.g., economic shifts or seasonality) might affect the results, introducing noise.  
Engineering Overhead: Longer experiments demand extended technical resources for monitoring and maintenance.  
Customer Confusion: If customers communicate and notice different prices, it could lead to dissatisfaction or complaints.

**5\. What are 3 iterations of the experiment that we could test as next steps?**  
Discount Bundles for Higher Price:  
   \- Offer optional services (e.g., an extra cleaning credit) to justify the $60 pricing for certain groups.  
   \- Test how the seen value impacts conversion and retention rates.  
Localized Pricing Strategy:  
   \- Test $60 in specific regions or demographics to explore the effect of local income levels and competitive dynamics on willingness to pay.  
Anchoring Strategy:  
   \- Offer a premium membership tier (e.g., $80/month with added benefits) alongside the $60 membership. This positions $60 as a mid-tier, more affordable option and tests customer behavior when given more choices.

# 

# supply-side experiment

**1\. What are some supply-side metrics that could indicate quality improvements?**  
Primary Metrics:

* Customer Satisfaction Ratings: Post-cleaning customer ratings (e.g., 1–5 stars).  
* Customer Complaint Rate: Percentage of cleanings that result in customer complaints or refund requests.  
* Repeated Booking Rate: Percentage of customers who book the same cleaner again.  
* Cleaning Completion Rate: Percentage of cleaning tasks completed as per customer expectations.

Secondary Metrics:

* Average Tip Amount: Indicates perceived service quality.  
* Task Adherence Rate: Percentage of tasks on the checklist completed and verified.  
* Cleaner Retention Rate: Measures whether cleaners using the checklist feel more supported and stay with the platform longer.NPS from Cleaners: Tracks cleaner satisfaction with the checklist process.

**2\. How could we connect supply-side metrics to a north-star metric like customer lifetime value (CLTV)?**

* Improved Satisfaction leads to Higher Retention: Higher customer satisfaction ratings often correlate with increased retention, directly increasing CLTV.  
* Reduced Complaints leads to Lower Churn: Fewer complaints lead to fewer refunds and a higher likelihood of repeat bookings, improving CLTV.  
* Rebooking Rate leads to an Increased Frequency: Customers who rebook the same cleaner are more likely to stay engaged with the platform, increasing the lifetime revenue from those customers.  
* Customer Referrals lower Lower customer acquisition costs : High-quality cleanings lead to positive word-of-mouth, reducing customer acquisition costs and improving overall profitability.

By using these intermediate metrics (e.g., satisfaction, rebooking rate), you can model and project their impact on CLTV.

**3\. How can we conduct an experiment to test this opportunity?**  
**Objective**: Assess whether a "to-do checklist" improves cleaning quality and downstream customer metrics.  
Sample Groups:

* Control Group: Cleaners do not receive or use a checklist.  
* Treatment Group: Cleaners use the to-do checklist.

Sample Sizing: Perform a power analysis to determine the sample size needed to detect a meaningful difference in primary metrics like satisfaction ratings or complaint rates.  
**Implementation Considerations:**

1. **Engineering**:

Integrate the checklist into the cleaner app or provide it in printed/digital format.  
Track whether cleaners access and complete the checklist. Record which group(checklist or no checklist) each cleaning belongs to.

2. **Training**:

Provide a brief tutorial for cleaners in the treatment group on using the checklist effectively.

3. **Analytics**:

Collect and centralize data from both groups, ensuring metrics like customer ratings, rebooking, and complaints are tied to individual cleanings.

4. **Monitoring & Adjustments:**

Monitor for any issues (e.g., technical problems with checklist access) during the experiment and resolve them promptly.  
**4\. How should we treat a data point where the customer is touched by a cleaner with a checklist and a cleaner without a checklist?**  
If possible, design the experiment to prevent this scenario by ensuring customers are consistently served by cleaners from the same sample group during the study. For example:  
Assign customers to either "checklist-only" or "no-checklist" cleaners.  
Use backend logic to match customers accordingly.  
**5\. What are 3 iterations of the experiment that we could test as next steps?**  
**Customized Checklists by Cleaning Type:**  
Test tailored checklists for different cleaning types (e.g., deep cleaning vs. standard cleaning) to determine if specialization improves quality.  
**Customer-Generated Checklists:**  
Allow customers to create or customize checklists for cleaners to follow. Measure the impact on satisfaction and rebooking rates.  
**Gamification for Cleaners:** Introduce rewards or recognition for cleaners who consistently adhere to the checklist and achieve high satisfaction ratings. Test the impact on cleaner engagement, retention, and quality.